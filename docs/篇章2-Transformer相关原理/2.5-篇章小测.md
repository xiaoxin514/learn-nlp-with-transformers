## 篇章小测
* 问题1: Transformer中的softmax计算为什么需要除以$d_k$?--缩放点积
* 问题2: Transformer中attention score计算时候如何mask掉padding位置？--在QK计算完成后，将mask位置赋予极大负数
* 问题3: 为什么Transformer中加入了positional embedding？--因为单词的Embedding没有包含位置信息，即上下文信息
* 问题4: BERT预训练时mask的比例，可以mask更大的比例吗？--可以更大的比例，但过大可能会导致信息缺失，难以学习上下文信息
* 问题5: BERT如何进行tokenize操作？有什么好处？--wrodPiece,解决未登录问题，平衡词表大小，更适合用于复杂语言
* 问题6: GPT如何进行tokenize操作？和BERT的区别是什么？--BPE,更注重完整语义单词，词表更大
* 问题7: BERT模型特别大，单张GPU训练仅仅只能放入1个batch的时候，怎么训练？
* 问题8: Transformer为什么需要一个position embedding？--因为单词的Embedding没有包含位置信息，即上下文信息
* 问题9: Transformer中的残差网络结构作用是什么？--缓解梯度小时，保留原始信息
* 问题10: BERT训练的时候mask单词的比例可以特别大（大于80%）吗？--不建议，过大可能会导致信息缺失，难以学习上下文信息
* 问题11: BERT预训练是如何做mask的？--随机mask15%
* 问题11: word2vec到BERT改进了什么？--动态获取上下文信息，BERT通过预训练+微调适用于多种下游任务，但word2vec只是用于捕获词向量，transformer更适合捕捉长距离语句关系
